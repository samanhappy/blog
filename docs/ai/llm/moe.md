# 混合专家架构（Mixture of Experts）

## 传统 MoE

混合专家架构（Mixture of Experts，简称 MoE）是一种先进的深度学习模型架构，旨在通过集成多个专业化的子模型——即“专家”——来解决复杂的问题。在这个架构中，每个专家都专注于其擅长的领域，并通过一个精妙的“门控网络”机制来协调它们的工作，决定哪个专家最适合处理特定的问题。

## LLM MoE

在大语言模型（LLM）领域，MoE 架构同样展现出了强大的潜力。它结合了多个具有各自优势和劣势的“专家”子模型，共同构建了一个更为稳健且多功能的全局模型。这种设计不仅提升了模型的整体性能，还有效地提高了效率。例如，GPT-4 就采用了这种架构，由 8 个较小的专家模型组成，每个模型都拥有高达 2200 亿的参数。

MoE 架构在预训练速度、推理速度以及成本方面均表现出显著优势。然而，它也面临着一些挑战，如对 GPU VRAM 的高需求以及相对复杂的训练过程。
